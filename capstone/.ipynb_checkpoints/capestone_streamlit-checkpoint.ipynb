{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6020e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running daily task at 2024-10-18 20:04:30.605000\n",
      "Iteration 1/32 completed.\n",
      "Iteration 2/32 completed.\n",
      "Iteration 3/32 completed.\n",
      "Iteration 4/32 completed.\n",
      "Iteration 5/32 completed.\n",
      "Iteration 6/32 completed.\n",
      "Iteration 7/32 completed.\n",
      "Iteration 8/32 completed.\n",
      "Iteration 9/32 completed.\n",
      "Iteration 10/32 completed.\n",
      "Iteration 11/32 completed.\n",
      "Iteration 12/32 completed.\n",
      "Iteration 13/32 completed.\n",
      "Iteration 14/32 completed.\n",
      "Iteration 15/32 completed.\n",
      "Iteration 16/32 completed.\n",
      "Iteration 17/32 completed.\n",
      "Iteration 18/32 completed.\n",
      "Iteration 19/32 completed.\n",
      "Iteration 20/32 completed.\n",
      "Iteration 21/32 completed.\n",
      "Iteration 22/32 completed.\n",
      "Iteration 23/32 completed.\n",
      "Iteration 24/32 completed.\n",
      "Iteration 25/32 completed.\n",
      "Iteration 26/32 completed.\n",
      "Iteration 27/32 completed.\n",
      "Iteration 28/32 completed.\n",
      "Iteration 29/32 completed.\n",
      "Iteration 30/32 completed.\n",
      "Iteration 31/32 completed.\n",
      "Iteration 32/32 completed.\n",
      "Data collection completed at {datetime.now()}\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 80441 entries, 2015-04-01 01:00:00 to 2024-10-18 20:00:00\n",
      "Data columns (total 19 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Site_Id          80441 non-null  int64  \n",
      " 1   Date             80441 non-null  object \n",
      " 2   Hour             80441 non-null  int64  \n",
      " 3   HourDescription  80441 non-null  object \n",
      " 4   CO               80441 non-null  float64\n",
      " 5   HUMID            80441 non-null  float64\n",
      " 6   NEPH             80441 non-null  float64\n",
      " 7   NO               80441 non-null  float64\n",
      " 8   NO2              80441 non-null  float64\n",
      " 9   OZONE            80441 non-null  float64\n",
      " 10  PM10             80441 non-null  float64\n",
      " 11  PM2.5            80441 non-null  float64\n",
      " 12  RAIN             80441 non-null  float64\n",
      " 13  SD1              80441 non-null  float64\n",
      " 14  SO2              80441 non-null  float64\n",
      " 15  SOLAR            80441 non-null  float64\n",
      " 16  TEMP             80441 non-null  float64\n",
      " 17  WDR              80441 non-null  float64\n",
      " 18  WSP              80441 non-null  float64\n",
      "dtypes: float64(15), int64(2), object(2)\n",
      "memory usage: 12.3+ MB\n",
      "Fetching air quality daily data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/kn0p5yrn6d771z_m2v7qll780000gp/T/ipykernel_83695/1741534129.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing traffic daily data...\n",
      "Fetching weather daily data...\n",
      "combining daily data...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0804 - val_loss: 0.0380\n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0511 - val_loss: 0.0358\n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 4s 44ms/step - loss: 0.0496 - val_loss: 0.0350\n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0471 - val_loss: 0.0324\n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0425 - val_loss: 0.0305\n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 4s 46ms/step - loss: 0.0393 - val_loss: 0.0280\n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0374 - val_loss: 0.0274\n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 4s 48ms/step - loss: 0.0360 - val_loss: 0.0281\n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0355 - val_loss: 0.0268\n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 4s 45ms/step - loss: 0.0352 - val_loss: 0.0279\n",
      "22/22 [==============================] - 0s 16ms/step\n",
      " 9/22 [===========>..................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 15ms/step\n",
      "modeling traffic data...\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "predicting traffic data...\n",
      "Epoch 1/20\n",
      " 3/88 [>.............................] - ETA: 3s - loss: 0.0488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0120 - val_loss: 0.0076\n",
      "Epoch 2/20\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0089 - val_loss: 0.0071\n",
      "Epoch 3/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0087 - val_loss: 0.0068\n",
      "Epoch 4/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0084 - val_loss: 0.0071\n",
      "Epoch 5/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0082 - val_loss: 0.0070\n",
      "Epoch 6/20\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 7/20\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 8/20\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0083 - val_loss: 0.0067\n",
      "Epoch 9/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0082 - val_loss: 0.0072\n",
      "Epoch 10/20\n",
      "88/88 [==============================] - 4s 44ms/step - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 11/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 12/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0080 - val_loss: 0.0068\n",
      "Epoch 13/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0081 - val_loss: 0.0069\n",
      "Epoch 14/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0079 - val_loss: 0.0068\n",
      "Epoch 15/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 16/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 17/20\n",
      "88/88 [==============================] - 4s 48ms/step - loss: 0.0078 - val_loss: 0.0068\n",
      "Epoch 18/20\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 19/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0077 - val_loss: 0.0066\n",
      "Epoch 20/20\n",
      "88/88 [==============================] - 4s 45ms/step - loss: 0.0076 - val_loss: 0.0069\n",
      "22/22 [==============================] - 0s 15ms/step\n",
      "RMSE (CO_mean): 0.07352588749570435\n",
      "RMSE (NO2_mean): 0.4197292649513087\n",
      "RMSE (NO_mean): 0.5097433896408695\n",
      "RMSE (OZONE_mean): 0.5669411785912126\n",
      "RMSE (PM10_mean): 5.845320851690189\n",
      "RMSE (PM2.5_mean): 3.6158409262123694\n",
      "RMSE (SO2_mean): 0.07988394082735262\n",
      "modeling air quality data...\n",
      "update weather data...\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "predecting air quality data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 20:26:54.375 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorise air quality data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 20:26:54.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.441 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.442 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-18 20:26:54.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update streamlit...\n",
      "Daily task completed at 2024-10-18 20:26:54.466525\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from ftplib import FTP\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error  # if you're using this elsewhere in the script\n",
    "from tensorflow.keras.models import load_model  # Import the load_model function to load the trained LSTM model\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "\n",
    "def run_daily_task():\n",
    "    print(f\"Running daily task at {datetime.now()}\")\n",
    "\n",
    "    # Define the base URL for the POST request\n",
    "    url = 'https://data.airquality.nsw.gov.au/api/Data/get_Observations'\n",
    "\n",
    "    # Define the initial start and end dates\n",
    "    start_date = datetime.strptime(\"2015-04-01\", \"%Y-%m-%d\")\n",
    "    end_date = start_date + timedelta(days=112)\n",
    "\n",
    "    # Set the number of iterations you want to loop through\n",
    "    num_iterations = 32  # Approximate number of iterations to get to today\n",
    "    retries = 3  # Number of retries for failed iterations\n",
    "\n",
    "    # Function to make the API request and process the data\n",
    "    def process_iteration(start_date_str, end_date_str):\n",
    "        # Construct the payload for each 112-day period\n",
    "        payload = {\n",
    "            \"Parameters\": [\"PM10\", \"PM2.5\", \"CO\", \"NH3\", \"NO\", \"NO2\", \"SO2\", \"OZONE\", \"TSPd\",\n",
    "                           \"RAIN\", \"SOLAR\", \"TEMP\", \"SD1\", \"WDR\", \"WSP\", \"Humid\", \"NEPH\"],                       \n",
    "            \"Sites\": [39],   # List of site IDs\n",
    "            \"StartDate\": start_date_str,  # Start date for the API request\n",
    "            \"EndDate\": end_date_str,      # End date for the API request\n",
    "            \"Categories\": [\"Averages\"],   \n",
    "            \"SubCategories\": [\"Hourly\"], \n",
    "            \"Frequency\": [\"Hourly Average\"]\n",
    "        }\n",
    "\n",
    "        # Set the headers for the request, if required\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the response JSON and convert it to a DataFrame\n",
    "            data = response.json()\n",
    "            df = pd.DataFrame(data)\n",
    "            return df  # Return the DataFrame if successful\n",
    "        else:\n",
    "            # Raise an exception if the request failed\n",
    "            raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "    # Collect DataFrames in a list to concatenate them later\n",
    "    data_frames = []\n",
    "\n",
    "    # Loop to go through each 56-day period and collect data\n",
    "    for i in range(num_iterations):\n",
    "        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        attempt = 0  # Track the number of attempts for each iteration\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                # Process the current iteration and append the DataFrame to the list\n",
    "                df = process_iteration(start_date_str, end_date_str)\n",
    "                data_frames.append(df)  # Store the DataFrame in the list\n",
    "                print(f\"Iteration {i + 1}/{num_iterations} completed.\")\n",
    "                break  # Exit the retry loop if the iteration is successful\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error on iteration {i + 1}: {e}\")\n",
    "                if attempt < retries:\n",
    "                    print(f\"Retrying iteration {i + 1} (Attempt {attempt}/{retries})...\")\n",
    "                    time.sleep(5)  # Wait 5 seconds before retrying\n",
    "                else:\n",
    "                    print(f\"Skipping iteration {i + 1} after {retries} failed attempts.\")\n",
    "\n",
    "        # Move forward by 56 days\n",
    "        start_date = end_date\n",
    "        end_date = start_date + timedelta(days=112)\n",
    "\n",
    "        # Sleep for 2 seconds between requests to avoid hitting rate limits\n",
    "        time.sleep(2)  # Pause for 2 seconds between iterations\n",
    "\n",
    "    # Concatenate all DataFrames at once after the loop\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # At this point, combined_df contains all the data\n",
    "    print(\"Data collection completed at {datetime.now()}\")\n",
    "\n",
    "\n",
    "    df=combined_df# Add your daily task logic here (e.g., API data fetching, processing, saving results)\n",
    "    \n",
    "\n",
    "    # Extract 'ParameterCode' and 'ParameterDescription' from the 'Parameter' column\n",
    "    # We handle cases where 'Parameter' might not be a dictionary\n",
    "    df['ParameterCode'] = df['Parameter'].apply(lambda x: x.get('ParameterCode') if isinstance(x, dict) else None)\n",
    "    df['ParameterDescription'] = df['Parameter'].apply(lambda x: x.get('ParameterDescription') if isinstance(x, dict) else None)\n",
    "\n",
    "    # Now pivot the DataFrame to have one row per time observation, with multiple columns\n",
    "    df_wide = df.pivot_table(index=['Site_Id', 'Date', 'Hour', 'HourDescription'],\n",
    "                             columns='ParameterCode', \n",
    "                             values='Value', \n",
    "                             aggfunc='first').reset_index()\n",
    "\n",
    "    # Flatten the MultiIndex columns if necessary\n",
    "    df_wide.columns = [col if not isinstance(col, tuple) else col[1] for col in df_wide.columns]\n",
    "    \n",
    "    # Fill NaNs at the edges with forward and backward filling\n",
    "    df_wide.fillna(method='ffill', inplace=True)\n",
    "    df_wide.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Then apply interpolation to fill NaNs between rows\n",
    "    df_wide.interpolate(method='linear', axis=0, inplace=True)\n",
    "\n",
    "    df_wide['datetime'] = pd.to_datetime(df_wide['Date']) + pd.to_timedelta(df_wide['Hour'], unit='h')\n",
    "\n",
    "    # Set 'datetime' as the index\n",
    "    df_wide.set_index('datetime', inplace=True)\n",
    "    df_wide.head()\n",
    "\n",
    "    # Define the aggregation rules for each parameter\n",
    "    aggregation_rules_mean = {\n",
    "        'CO': 'mean',           # Daily mean for CO\n",
    "        'HUMID': 'mean',        # Daily mean for Humidity\n",
    "        'NEPH': 'mean',         # Daily mean for NEPH\n",
    "        'NO': 'mean',           # Daily mean for NO\n",
    "        'NO2': 'mean',          # Daily mean for NO2\n",
    "        'OZONE': 'mean',        # Daily mean for Ozone\n",
    "        'SO2': 'mean',          # Daily mean for SO2\n",
    "        'PM10': 'mean',         # Daily mean for PM10\n",
    "        'PM2.5': 'mean',        # Daily mean for PM2.5\n",
    "        'RAIN': 'sum',          # Total rainfall for the day\n",
    "        'TEMP': ['min', 'max'], # Min and Max for temperature\n",
    "        'WSP': 'max',           # Max wind speed for the day\n",
    "        'SD1': 'mean',          # Mean wind direction 1\n",
    "        'WDR': 'mean',          # Mean wind direction (special handling can be added)\n",
    "    }\n",
    "\n",
    "    df_wide_filled = df_wide.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    df_wide.info()\n",
    "\n",
    "    # Now resample the data into daily data based on the rules above\n",
    "    daily_aggregated_mean = df_wide.resample('D').agg(aggregation_rules_mean)\n",
    "\n",
    "    # Flatten the MultiIndex columns if needed\n",
    "    daily_aggregated_mean.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in daily_aggregated_mean.columns]\n",
    "\n",
    "    daily_aggregated_mean.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    daily_aggregated_mean.head()\n",
    "    \n",
    "    print(\"Fetching air quality daily data...\")\n",
    "\n",
    "     # Load the data\n",
    "    df = pd.read_csv('traffic_victoria_road.csv')\n",
    "\n",
    "    # Melt the DataFrame to convert the hour columns into rows\n",
    "    df_melted = pd.melt(df, \n",
    "                        id_vars=['year', 'date', 'cardinal_direction_seq', 'classification_seq', 'public_holiday', 'school_holiday'],\n",
    "                        value_vars=[f'hour_{str(i).zfill(2)}' for i in range(24)],\n",
    "                        var_name='hour', \n",
    "                        value_name='traffic_count')\n",
    "    df_melted.head()\n",
    "    \n",
    "    # Clean up the 'hour' column (convert 'hour_00' to '00', 'hour_01' to '01', etc.)\n",
    "    df_melted['hour'] = df_melted['hour'].str.replace('hour_', '').astype(int)\n",
    "\n",
    "    # Pivot the DataFrame to separate 'Heavy Vehicles' and 'Light Vehicles' into their own columns\n",
    "    df_pivoted = df_melted.pivot_table(index=['year', 'date', 'hour', 'public_holiday', 'school_holiday'],\n",
    "                                       columns='classification_seq', \n",
    "                                       values='traffic_count').reset_index()\n",
    "\n",
    "    # Rename the columns for clarity\n",
    "    df_pivoted.columns.name = None  # Remove the pivot table's automatic column grouping name\n",
    "    df_pivoted.rename(columns={'Heavy Vehicles': 'heavy_vehicle', 'Light Vehicles': 'light_vehicle'}, inplace=True)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    df_clean = df_pivoted.dropna()\n",
    "\n",
    "    # Combine 'date' and 'hour' columns into a single 'datetime' column\n",
    "    df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n",
    "\n",
    "    # Set 'datetime' as the index\n",
    "    df_clean = df_clean.set_index('datetime')\n",
    "\n",
    "    # Resample the data to daily frequency\n",
    "    daily_traffic_data = df_clean.resample('D').agg({\n",
    "        'public_holiday': 'max',  # Taking the maximum value for the day\n",
    "        'school_holiday': 'max',  # Taking the maximum value for the day\n",
    "        'heavy_vehicle': 'sum',   # Summing heavy vehicle counts\n",
    "        'light_vehicle': 'sum'    # Summing light vehicle counts\n",
    "    })\n",
    "\n",
    "\n",
    "    # Create a list of school term date ranges for 2020 to 2025\n",
    "    school_term_dates = [\n",
    "        # 2020 Term dates (Eastern and Western division)\n",
    "        ('2020-01-28', '2020-04-09'),  # Term 1 Eastern\n",
    "        ('2020-02-04', '2020-04-09'),  # Term 1 Western\n",
    "        ('2020-04-27', '2020-07-03'),  # Term 2\n",
    "        ('2020-07-20', '2020-09-25'),  # Term 3\n",
    "        ('2020-10-12', '2020-12-18'),  # Term 4\n",
    "        # 2021 Term dates\n",
    "        ('2021-01-27', '2021-04-01'),  # Term 1 Eastern\n",
    "        ('2021-02-03', '2021-04-01'),  # Term 1 Western\n",
    "        ('2021-04-19', '2021-06-25'),  # Term 2\n",
    "        ('2021-07-12', '2021-09-17'),  # Term 3\n",
    "        ('2021-10-05', '2021-12-17'),  # Term 4\n",
    "        # 2022 Term dates\n",
    "        ('2022-01-28', '2022-04-08'),  # Term 1 Eastern\n",
    "        ('2022-02-04', '2022-04-08'),  # Term 1 Western\n",
    "        ('2022-04-26', '2022-07-01'),  # Term 2\n",
    "        ('2022-07-18', '2022-09-23'),  # Term 3\n",
    "        ('2022-10-10', '2022-12-20'),  # Term 4\n",
    "        # 2023 Term dates\n",
    "        ('2023-01-27', '2023-04-06'),  # Term 1 Eastern\n",
    "        ('2023-02-03', '2023-04-06'),  # Term 1 Western\n",
    "        ('2023-04-24', '2023-06-30'),  # Term 2\n",
    "        ('2023-07-17', '2023-09-22'),  # Term 3\n",
    "        ('2023-10-09', '2023-12-19'),  # Term 4\n",
    "        # 2024 Term dates\n",
    "        ('2024-02-01', '2024-04-12'),  # Term 1 Eastern\n",
    "        ('2024-02-08', '2024-04-12'),  # Term 1 Western\n",
    "        ('2024-04-30', '2024-07-05'),  # Term 2\n",
    "        ('2024-07-23', '2024-09-27'),  # Term 3\n",
    "        ('2024-10-14', '2024-12-18'),  # Term 4\n",
    "        # 2025 Term dates\n",
    "        ('2025-02-04', '2025-04-11'),  # Term 1 Eastern\n",
    "        ('2025-02-11', '2025-04-11'),  # Term 1 Western\n",
    "        ('2025-04-30', '2025-07-04'),  # Term 2\n",
    "        ('2025-07-22', '2025-09-26'),  # Term 3\n",
    "        ('2025-10-13', '2025-12-19')   # Term 4\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame for school term dates with 0 as school days\n",
    "    df_school_term = pd.DataFrame({\n",
    "        'Start': pd.to_datetime([start for start, end in school_term_dates]),\n",
    "        'End': pd.to_datetime([end for start, end in school_term_dates]),\n",
    "        'school_holiday': 0  # School days are 0\n",
    "    })\n",
    "\n",
    "    # Create a date range DataFrame from 2020 to 2025\n",
    "    date_range = pd.date_range(start='2020-01-01', end='2025-12-31', freq='D')\n",
    "    df_dates = pd.DataFrame(date_range, columns=['datetime'])\n",
    "    df_dates['school_holiday'] = 1  # Default to 1 (holidays)\n",
    "\n",
    "    # Mark the term dates (non-holidays) as 0\n",
    "    for _, row in df_school_term.iterrows():\n",
    "        df_dates.loc[(df_dates['datetime'] >= row['Start']) & (df_dates['datetime'] <= row['End']), 'school_holiday'] = 0\n",
    "\n",
    "    # Data for public holidays from 2020 to 2025\n",
    "    public_holidays = {\n",
    "        'holiday': [\n",
    "            \"New Year's Day\", \"Australia Day\", \"Good Friday\", \"Easter Saturday\", \"Easter Sunday\", \n",
    "            \"Easter Monday\", \"Anzac Day\", \"King's Birthday\", \"Labour Day\", \"Christmas Day\", \"Boxing Day\"\n",
    "        ],\n",
    "        '2020': [\n",
    "            '2020-01-01', '2020-01-27', '2020-04-10', '2020-04-11', '2020-04-12', \n",
    "            '2020-04-13', '2020-04-25', '2020-06-08', '2020-10-05', '2020-12-25', '2020-12-26'\n",
    "        ],\n",
    "        '2021': [\n",
    "            '2021-01-01', '2021-01-26', '2021-04-02', '2021-04-03', '2021-04-04', \n",
    "            '2021-04-05', '2021-04-25', '2021-06-14', '2021-10-04', '2021-12-25', '2021-12-26'\n",
    "        ],\n",
    "        '2022': [\n",
    "            '2022-01-01', '2022-01-26', '2022-04-15', '2022-04-16', '2022-04-17', \n",
    "            '2022-04-18', '2022-04-25', '2022-06-13', '2022-10-03', '2022-12-25', '2022-12-26'\n",
    "        ],\n",
    "        '2023': [\n",
    "            '2023-01-01', '2023-01-26', '2023-04-07', '2023-04-08', '2023-04-09', \n",
    "            '2023-04-10', '2023-04-25', '2023-06-12', '2023-10-02', '2023-12-25', '2023-12-26'\n",
    "        ],\n",
    "        '2024': [\n",
    "            '2024-01-01', '2024-01-26', '2024-03-29', '2024-03-30', '2024-03-31', \n",
    "            '2024-04-01', '2024-04-25', '2024-06-10', '2024-10-07', '2024-12-25', '2024-12-26'\n",
    "        ],\n",
    "        '2025': [\n",
    "            '2025-01-01', '2025-01-27', '2025-04-18', '2025-04-19', '2025-04-20', \n",
    "            '2025-04-21', '2025-04-25', '2025-06-09', '2025-10-06', '2025-12-25', '2025-12-26'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Convert public holiday data to a DataFrame\n",
    "    df_holidays = pd.DataFrame(public_holidays)\n",
    "    df_holidays_melted = df_holidays.melt(id_vars=['holiday'], var_name='Year', value_name='Date')\n",
    "    df_holidays_melted = df_holidays_melted.dropna()\n",
    "    df_holidays_melted['Date'] = pd.to_datetime(df_holidays_melted['Date'])\n",
    "    df_holidays_melted['public_holiday'] = 1\n",
    "\n",
    "    # Merge public holidays into the date range DataFrame\n",
    "    df_final = pd.merge(df_dates, df_holidays_melted[['Date', 'public_holiday']], left_on='datetime', right_on='Date', how='left')\n",
    "    df_final['public_holiday'] = df_final['public_holiday'].fillna(0).astype(int)\n",
    "\n",
    "    # Drop unnecessary 'Date' column\n",
    "    df_holiday = df_final.drop(columns=['Date'])\n",
    "\n",
    "\n",
    "    # Get the first date in df_holiday\n",
    "    first_date = df_holiday.index.min()\n",
    "\n",
    "    df_holiday.set_index('datetime', inplace=True)\n",
    "    #daily_traffic_data.set_index('datetime', inplace=True)\n",
    "\n",
    "\n",
    "    # Combine both DataFrames to include all rows from df_holiday that are not present in daily_traffic_data\n",
    "    # This will ensure that missing dates are added\n",
    "    daily_traffic_data = daily_traffic_data.combine_first(df_holiday)\n",
    "\n",
    "    # Update the 'school_holiday' and 'public_holiday' columns with values from df_holiday\n",
    "    daily_traffic_data.update(df_holiday[['school_holiday', 'public_holiday']])\n",
    "\n",
    "\n",
    "    # Using 'outer' join to ensure we keep all dates across the three DataFrames\n",
    "    combined_data = pd.concat([daily_aggregated_mean, daily_traffic_data], axis=1, join='outer')\n",
    "\n",
    "    # Drop the first row of the DataFrame\n",
    "    combined_data = combined_data.iloc[1:]\n",
    "\n",
    "    # Find the last entry (non-NaN) for 'heavy_vehicle'\n",
    "    last_entry_date = combined_data['heavy_vehicle'].last_valid_index()\n",
    "\n",
    "    # Forward fill NaN values for all columns between the second row and the last valid entry for 'heavy_vehicle'\n",
    "    combined_data.loc[:last_entry_date] = combined_data.loc[:last_entry_date].fillna(method='ffill')\n",
    "   \n",
    "    # Using 'outer' join to ensure we keep all dates across the three DataFrames\n",
    "    combined_data = pd.concat([daily_aggregated_mean, daily_traffic_data], axis=1, join='outer')\n",
    "\n",
    "    # Drop the first row of the DataFrame\n",
    "    combined_data = combined_data.iloc[1:]\n",
    "\n",
    "    # Find the last entry (non-NaN) for 'heavy_vehicle'\n",
    "    last_entry_date = combined_data['heavy_vehicle'].last_valid_index()\n",
    "\n",
    "    # Forward fill NaN values for all columns between the second row and the last valid entry for 'heavy_vehicle'\n",
    "    combined_data.loc[:last_entry_date] = combined_data.loc[:last_entry_date].fillna(method='ffill')\n",
    "    \n",
    "    # Example: Save or process data\n",
    "    print(\"Processing traffic daily data...\")\n",
    "    \n",
    "    # Connect to FTP and download the XML file\n",
    "    ftp = FTP('ftp.bom.gov.au')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/anon/gen/fwo/')\n",
    "    with open('IDN11060.xml', 'wb') as file:\n",
    "        ftp.retrbinary('RETR IDN11060.xml', file.write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse('IDN11060.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Prepare an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each 'area' element to get all forecast data\n",
    "    for area in root.findall('.//area'):\n",
    "        location = area.attrib.get('description')\n",
    "\n",
    "        # Loop through each forecast period in the location\n",
    "        for period in area.findall('.//forecast-period'):\n",
    "            start_time = period.attrib.get('start-time-local')\n",
    "\n",
    "            # Initialize a dictionary to hold the data for this forecast period\n",
    "            forecast_data = {\n",
    "                'Location': location,\n",
    "                'Date': start_time\n",
    "            }\n",
    "\n",
    "            # Loop through all 'element' tags\n",
    "            for element in period.findall('element'):\n",
    "                param_type = element.attrib.get('type')\n",
    "                value = element.text\n",
    "                units = element.attrib.get('units', '')\n",
    "                forecast_data[f'{param_type} ({units})'] = value\n",
    "\n",
    "            # Loop through all 'text' tags\n",
    "            for text in period.findall('text'):\n",
    "                text_type = text.attrib.get('type')\n",
    "                text_value = text.text\n",
    "                forecast_data[text_type] = text_value\n",
    "\n",
    "            # Append the forecast data to the list\n",
    "            data.append(forecast_data)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = df[['Location', 'Date', \n",
    "                 'air_temperature_maximum (Celsius)', 'air_temperature_minimum (Celsius)', \n",
    "                 'precipitation_range ()']].copy()\n",
    "\n",
    "    # Renaming the columns to the desired format\n",
    "    df.columns = ['Location', 'Date', 'Temp_Max', 'Temp_Min', 'Rain']\n",
    "\n",
    "    def extract_rain_value(rain):\n",
    "        if pd.isnull(rain):\n",
    "            return 0  # If NaN, return 0\n",
    "        # Extract the larger number from strings like \"0 to 2 mm\"\n",
    "        rain_values = re.findall(r'\\d+', rain)\n",
    "        if rain_values:\n",
    "            return int(rain_values[-1])  # Return the largest number (last in the list)\n",
    "        return 0  # Default to 0 if no numbers are found\n",
    "\n",
    "    # Apply the extraction function to the 'Rain' column\n",
    "    df['Rain'] = df['Rain'].apply(extract_rain_value)\n",
    "\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Convert 'Temp_Max' and 'Temp_Min' columns to numeric, forcing errors to NaN if conversion fails\n",
    "    df['Temp_Max'] = pd.to_numeric(df['Temp_Max'], errors='coerce')\n",
    "    df['Temp_Min'] = pd.to_numeric(df['Temp_Min'], errors='coerce')\n",
    "\n",
    "    # Calculating the difference between Temp_Max and Temp_Min for each row\n",
    "    df['Temp_Diff'] = df['Temp_Max'] - df['Temp_Min']\n",
    "\n",
    "    # Calculating the average difference between Temp_Max and Temp_Min (excluding NaN values)\n",
    "    avg_temp_diff = df['Temp_Diff'].mean()\n",
    "\n",
    "    # Filling the NaN values in Temp_Min by subtracting the average temperature difference from Temp_Max\n",
    "    df['Temp_Min'].fillna(df['Temp_Max'] - avg_temp_diff, inplace=True)\n",
    "\n",
    "    # Dropping the Temp_Diff column as it's no longer needed\n",
    "    df.drop(columns=['Temp_Diff'], inplace=True)\n",
    "\n",
    "    df_sydney = df[df['Location'] == 'Sydney'].copy()\n",
    "    # Connect to FTP and download the XML file\n",
    "    ftp = FTP('ftp.bom.gov.au')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/anon/gen/fwo/')\n",
    "    with open('IDN11050.xml', 'wb') as file:\n",
    "        ftp.retrbinary('RETR IDN11050.xml', file.write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse('IDN11050.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Prepare an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each 'area' element to get all forecast data\n",
    "    for area in root.findall('.//area'):\n",
    "        location = area.attrib.get('description')\n",
    "\n",
    "        # Loop through each forecast period in the location\n",
    "        for period in area.findall('.//forecast-period'):\n",
    "            start_time = period.attrib.get('start-time-local')\n",
    "\n",
    "            # Initialize a dictionary to hold the data for this forecast period\n",
    "            forecast_data = {\n",
    "                'Location': location,\n",
    "                'Date': start_time\n",
    "            }\n",
    "\n",
    "            # Loop through all 'element' tags\n",
    "            for element in period.findall('element'):\n",
    "                param_type = element.attrib.get('type')\n",
    "                value = element.text\n",
    "                units = element.attrib.get('units', '')\n",
    "                forecast_data[f'{param_type} ({units})'] = value\n",
    "\n",
    "            # Loop through all 'text' tags\n",
    "            for text in period.findall('text'):\n",
    "                text_type = text.attrib.get('type')\n",
    "                text_value = text.text\n",
    "                forecast_data[text_type] = text_value\n",
    "\n",
    "            # Append the forecast data to the list\n",
    "            data.append(forecast_data)\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df_wind = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    # Mapping wind direction text to degrees\n",
    "    wind_direction_degrees = {\n",
    "        'northerly': 0, 'north to northeasterly': 22.5, 'northeasterly': 45, 'east to northeasterly': 67.5,\n",
    "        'easterly': 90, 'east to southeasterly': 112.5, 'southeast': 135, 'south to southeast': 157.5,\n",
    "        'southerly': 180, 'south to southwesterly': 202.5, 'southwesterly': 225, 'west-southwesterly': 247.5,\n",
    "        'westerly': 270, 'west to northwesterly': 292.5, 'northwesterly': 315, 'north to northwesterly': 337.5\n",
    "    }\n",
    "\n",
    "    # Function to extract wind speed and direction from the 'Forecast' column\n",
    "    def extract_wind_info(forecast):\n",
    "        if pd.isnull(forecast):\n",
    "            return None, None, None  # Handle None or NaN values\n",
    "\n",
    "\n",
    "        # Regular expression to handle both \"Winds ...\" and \"becoming ...\"\n",
    "        wind_info = re.search(r'(Winds|becoming)\\s([a-zA-Z\\s]+)\\s(\\d+)(?:\\sto\\s(\\d+))?\\skm/h', forecast)\n",
    "\n",
    "        if wind_info:\n",
    "            wind_direction = wind_info.group(2).strip().lower()\n",
    "            wind_speed_min = int(wind_info.group(3))\n",
    "            wind_speed_max = int(wind_info.group(4)) if wind_info.group(4) else None\n",
    "\n",
    "            # Convert the wind direction to degrees if it's in the mapping\n",
    "            for direction, degrees in wind_direction_degrees.items():\n",
    "                if direction in wind_direction:\n",
    "                    wind_direction_degrees_value = degrees\n",
    "                    break\n",
    "            else:\n",
    "                wind_direction_degrees_value = None  # Handle unknown direction\n",
    "\n",
    "            return wind_direction_degrees_value, wind_speed_min, wind_speed_max\n",
    "\n",
    "        return None, None, None\n",
    "\n",
    "    # Apply the function to the 'Forecast' column and store the results in new columns\n",
    "    df_wind['Wind Direction (Degrees)'], df_wind['Wind Speed Min'], df_wind['Wind Speed Max'] = zip(*df_wind['forecast'].apply(extract_wind_info))\n",
    "\n",
    "    def kmh_to_ms(speed_kmh):\n",
    "        return round(speed_kmh * 0.27778, 2) if speed_kmh is not None else None\n",
    "\n",
    "    # Apply the conversion after extracting the wind data\n",
    "    df_wind['Wind Speed Min (m/s)'] = df_wind['Wind Speed Min'].apply(kmh_to_ms)\n",
    "    df_wind['Wind Speed Max (m/s)'] = df_wind['Wind Speed Max'].apply(kmh_to_ms)\n",
    "    df_wind['Date'] = pd.to_datetime(df_wind['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Display the extracted wind information\n",
    "    df_wind=df_wind[['Location',\n",
    "                     'Date','Wind Direction (Degrees)', \n",
    "                     'Wind Speed Min (m/s)', \n",
    "                     'Wind Speed Max (m/s)',\n",
    "                     'forecast']]\n",
    "\n",
    "    df_wind_sydney = df_wind[df_wind['Location'] == 'Sydney'][['Date', \n",
    "                                                               'Wind Direction (Degrees)', \n",
    "                                                               'Wind Speed Min (m/s)', \n",
    "                                                               'Wind Speed Max (m/s)',\n",
    "                                                               'forecast']].copy()\n",
    "\n",
    "    df_forecast_sydney = pd.merge(df_sydney, df_wind_sydney, on='Date', how='inner')\n",
    "\n",
    "    df_forecast_sydney = df_forecast_sydney.drop(columns=['Location'])  # Drop the \"Location\" column\n",
    "    df_forecast_sydney['Date'] = pd.to_datetime(df_forecast_sydney['Date'])  # Convert 'Date' column to datetime\n",
    "    df_forecast_sydney = df_forecast_sydney.set_index('Date')  # Set 'Date' as the index\n",
    "\n",
    "\n",
    "    df_forecast_sydney.rename(columns={\n",
    "        'Rain': 'RAIN_sum',\n",
    "        'Wind Direction (Degrees)': 'WDR_mean',\n",
    "        'Wind Speed Max (m/s)': 'WSP_max',\n",
    "        'Temp_Max': 'TEMP_max',\n",
    "        'Temp_Min': 'TEMP_min',\n",
    "        'forecast': 'forecast'\n",
    "    }, inplace=True)\n",
    "\n",
    "    df_7forecast_sydney = df_forecast_sydney[['TEMP_max', \n",
    "                                              'TEMP_min', \n",
    "                                              'RAIN_sum', \n",
    "                                              'WSP_max', \n",
    "                                              'WDR_mean',\n",
    "                                             'forecast']]\n",
    "    \n",
    "    df_7forecast_sydney.head()\n",
    "    \n",
    "    print(\"Fetching weather daily data...\")\n",
    "\n",
    "    # Ensure both DataFrames have their indices in datetime format\n",
    "    combined_data.index = pd.to_datetime(combined_data.index)\n",
    "    df_7forecast_sydney.index = pd.to_datetime(df_7forecast_sydney.index)\n",
    "\n",
    "    # Remove any overlapping dates between the indices of combined_data and df_7forecast_sydney\n",
    "    combined_data = combined_data[~combined_data.index.isin(df_7forecast_sydney.index)]\n",
    "\n",
    "    # Combine df_7forecast_sydney into combined_data, updating existing columns\n",
    "    combined_data = pd.concat([combined_data, df_7forecast_sydney], axis=0, join='outer')\n",
    "\n",
    "    # Sort by index (date)\n",
    "    combined_data = combined_data.sort_index()\n",
    "\n",
    "    # Drop the first row of the DataFrame if needed\n",
    "    combined_data = combined_data.iloc[1:]\n",
    "\n",
    "    # Find the last entry (non-NaN) for 'heavy_vehicle'\n",
    "    last_entry_date = combined_data['heavy_vehicle'].last_valid_index()\n",
    "\n",
    "    # Forward fill NaN values for all columns between the second row and the last valid entry for 'heavy_vehicle'\n",
    "    combined_data.loc[:last_entry_date] = combined_data.loc[:last_entry_date].fillna(method='ffill')\n",
    "\n",
    "    # Display the final combined DataFrame to check the result\n",
    "\n",
    "\n",
    "    print(\"combining daily data...\")\n",
    "\n",
    "    # Select the features to include in the model (in addition to 'heavy_vehicle' and 'light_vehicle')\n",
    "    features = ['heavy_vehicle', 'light_vehicle','RAIN_sum']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(combined_data[features])  # Scale selected columns\n",
    "\n",
    "    # Create a new DataFrame with the scaled data and the same index\n",
    "    combined_data_scaled = pd.DataFrame(scaled_data, columns=features, index=combined_data.index)\n",
    "\n",
    "    #  Handle missing values\n",
    "    # Find the first missing value in either 'heavy_vehicle' or 'light_vehicle'\n",
    "    first_missing_index = combined_data[['heavy_vehicle', 'light_vehicle']].isnull().idxmax().max()\n",
    "\n",
    "    # Convert the first_missing_index to its integer location\n",
    "    first_missing_loc = combined_data.index.get_loc(first_missing_index)\n",
    "\n",
    "    # Filter data up to the first missing value\n",
    "    combined_data_filtered = combined_data_scaled.iloc[:first_missing_loc] \n",
    "\n",
    "    # Create sequences (LSTM expects a 3D input: [samples, timesteps, features])\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])  # Input sequence\n",
    "            y.append(data[i, :2])           # Target for heavy_vehicle (0) and light_vehicle (1)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Define the sequence length and reshape the data for LSTM\n",
    "    sequence_length = 7  # 7-day sequences\n",
    "    data = combined_data_filtered.values  # Use scaled and filtered data with multiple features\n",
    "    X, y = create_sequences(data, sequence_length)\n",
    "\n",
    "    # Split the data into training and testing data\n",
    "    split_index = int(len(combined_data_filtered) * 0.8)  # Use 80% for training, 20% for testing\n",
    "\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Define the LSTM model (with multiple input features and two outputs for heavy_vehicle and light_vehicle)\n",
    "    model = Sequential()\n",
    "    # Adjust input shape to account for the number of features (len(features))\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, len(features))))  # Use all selected features\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(2))  # Predict two values at a time (for heavy_vehicle and light_vehicle)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the LSTM model to predict both heavy_vehicle and light_vehicle\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Apply MinMaxScaler only for heavy_vehicle and light_vehicle\n",
    "    scaler_vehicle = MinMaxScaler()\n",
    "    scaler_vehicle.fit(combined_data[['heavy_vehicle', 'light_vehicle']])  # Fit scaler on only these two columns\n",
    "\n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and actual values (for comparison)\n",
    "    y_pred_inverse = scaler_vehicle.inverse_transform(y_pred)\n",
    "    y_test_inverse = scaler_vehicle.inverse_transform(y_test)\n",
    "\n",
    "    # Calculate RMSE and MAE for both heavy_vehicle and light_vehicle (same as before)\n",
    "    rmse_heavy_vehicle = np.sqrt(mean_squared_error(y_test_inverse[:, 0], y_pred_inverse[:, 0]))\n",
    "    rmse_light_vehicle = np.sqrt(mean_squared_error(y_test_inverse[:, 1], y_pred_inverse[:, 1]))\n",
    "\n",
    "    mae_heavy_vehicle = mean_absolute_error(y_test_inverse[:, 0], y_pred_inverse[:, 0])\n",
    "    mae_light_vehicle = mean_absolute_error(y_test_inverse[:, 1], y_pred_inverse[:, 1])\n",
    "\n",
    "    print(\"modeling traffic data...\")\n",
    "\n",
    "    # Prepare to make rolling predictions from the first missing value until 8 days from today\n",
    "    today = pd.Timestamp.today()\n",
    "    days_to_predict = (today - first_missing_index).days + 8  # From first missing value to 8 days from today\n",
    "\n",
    "    # Prepare the last valid sequence for prediction\n",
    "    # Assume the model was trained on a sequence of 7 days and all features were used in training\n",
    "    last_sequence = combined_data_scaled.iloc[first_missing_loc-sequence_length:first_missing_loc].values\n",
    "    pred_input = np.array([last_sequence])\n",
    "\n",
    "    # Predict for all missing days and 8 days into the future (Rolling Forecast)\n",
    "    predictions_future = []\n",
    "    for _ in range(days_to_predict):\n",
    "        # Predict one step ahead\n",
    "        pred = model.predict(pred_input)\n",
    "        predictions_future.append(pred[0])  # Store the prediction for the current step\n",
    "\n",
    "        # Update pred_input by appending the prediction and removing the first timestep\n",
    "        pred_input = np.append(pred_input[:, 1:, :], [[np.hstack([pred_input[0, -1, 2:], pred[0]])]], axis=1)\n",
    "\n",
    "    # Inverse transform the predictions ONLY for heavy_vehicle and light_vehicle\n",
    "    # Assuming the scaler was originally fitted only on 'heavy_vehicle' and 'light_vehicle' columns\n",
    "    predictions_future = np.array(predictions_future)\n",
    "\n",
    "    # Perform the inverse transform only on the 2 columns (heavy_vehicle, light_vehicle)\n",
    "    scaler_vehicle = MinMaxScaler()\n",
    "    scaler_vehicle.fit(combined_data[['heavy_vehicle', 'light_vehicle']])  # Fit the scaler only on relevant columns\n",
    "    predictions_future_scaled = scaler_vehicle.inverse_transform(predictions_future[:, :2])  # Inverse-transform only the 2 columns\n",
    "\n",
    "    # Create a DataFrame of the predicted values\n",
    "    predicted_dates = pd.date_range(start=first_missing_index, periods=days_to_predict)\n",
    "    predicted_values_df = pd.DataFrame(predictions_future_scaled, columns=['heavy_vehicle', 'light_vehicle'], index=predicted_dates)\n",
    "\n",
    "    # Update the original combined_data DataFrame with the predictions\n",
    "    combined_data.update(predicted_values_df)\n",
    "\n",
    "    combined_data.loc['2024-10-15']\n",
    "    \n",
    "    print(\"predicting traffic data...\")\n",
    "\n",
    "    \n",
    "    # Prepare the data\n",
    "    features = ['TEMP_min', 'TEMP_max', 'RAIN_sum', 'WDR_mean', 'WSP_max', 'heavy_vehicle', 'light_vehicle']\n",
    "    target = ['CO_mean', 'NO2_mean', 'NO_mean', 'OZONE_mean', 'PM10_mean', 'PM2.5_mean', 'SO2_mean']\n",
    "\n",
    "    # Scale the features and target variables\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaled_features = scaler_features.fit_transform(combined_data[features])\n",
    "\n",
    "    scaler_target = MinMaxScaler()\n",
    "    scaled_target = scaler_target.fit_transform(combined_data[target])\n",
    "\n",
    "    # Combine the scaled features and targets into a single DataFrame\n",
    "    combined_data_scaled = pd.DataFrame(np.hstack([scaled_features, scaled_target]), \n",
    "                                        columns=features + target, \n",
    "                                        index=combined_data.index)\n",
    "\n",
    "    # Handle missing values\n",
    "    first_missing_index = combined_data[target].isnull().idxmax().max()\n",
    "    first_missing_loc = combined_data.index.get_loc(first_missing_index)\n",
    "\n",
    "    # Filter data up to the first missing value\n",
    "    combined_data_filtered = combined_data_scaled.iloc[:first_missing_loc]\n",
    "\n",
    "    # Create sequences (LSTM expects a 3D input: [samples, timesteps, features])\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i, :len(features)])  # Input sequence (features)\n",
    "            y.append(data[i, len(features):])              # Target sequence (CO, NO2, etc.)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Define the sequence length (7-day sequences)\n",
    "    sequence_length = 7\n",
    "    data = combined_data_filtered.values\n",
    "    X, y = create_sequences(data, sequence_length)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    split_index = int(len(combined_data_filtered) * 0.8)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Build and compile the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, len(features))))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(len(target)))  # Output layer for the 7 target variables\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the LSTM model\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predicted and actual values for comparison\n",
    "    y_pred_inverse = scaler_target.inverse_transform(y_pred)\n",
    "    y_test_inverse = scaler_target.inverse_transform(y_test)\n",
    "\n",
    "    # Calculate RMSE for each target variable\n",
    "    for i, target_name in enumerate(target):\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_inverse[:, i], y_pred_inverse[:, i]))\n",
    "        print(f\"RMSE ({target_name}): {rmse}\")\n",
    "\n",
    "    # Save the model for the prediction step\n",
    "    model.save(\"lstm_air_quality_model.h5\")\n",
    "    np.save(\"scaler_features.npy\", scaler_features)\n",
    "    np.save(\"scaler_target.npy\", scaler_target)\n",
    "\n",
    "    print(\"modeling air quality data...\")\n",
    "\n",
    "    # Ensure both DataFrames have their indices in datetime format\n",
    "    combined_data.index = pd.to_datetime(combined_data.index)\n",
    "    df_7forecast_sydney.index = pd.to_datetime(df_7forecast_sydney.index)\n",
    "\n",
    "    # Overwrite only the rows in combined_data where the dates overlap with df_7forecast_sydney\n",
    "    # This will keep all existing data in combined_data except for the overlapping dates, which will be replaced\n",
    "    combined_data.update(df_7forecast_sydney)\n",
    "\n",
    "    # Optionally, sort the DataFrame by the index to ensure the data is in chronological order\n",
    "    combined_data = combined_data.sort_index()\n",
    "\n",
    "    # Drop the first row of the DataFrame if needed\n",
    "    combined_data = combined_data.iloc[1:]\n",
    "\n",
    "    # Find the last valid entry for 'heavy_vehicle'\n",
    "    last_entry_date = combined_data['heavy_vehicle'].last_valid_index()\n",
    "\n",
    "    # Forward fill NaN values for all columns up to the last valid entry for 'heavy_vehicle'\n",
    "    combined_data.loc[:last_entry_date] = combined_data.loc[:last_entry_date].fillna(method='ffill')\n",
    "\n",
    "\n",
    "    print(\"update weather data...\")\n",
    "\n",
    "\n",
    "    # Load the trained model and scalers\n",
    "    model = load_model(\"lstm_air_quality_model.h5\")\n",
    "    scaler_features = np.load(\"scaler_features.npy\", allow_pickle=True).item()\n",
    "    scaler_target = np.load(\"scaler_target.npy\", allow_pickle=True).item()\n",
    "\n",
    "    # Rolling forecast function\n",
    "    def rolling_forecast(model, initial_sequence, n_days, scaler_features, scaler_target):\n",
    "        rolling_predictions = []\n",
    "        current_sequence = initial_sequence  # Start with the most recent sequence\n",
    "\n",
    "        for _ in range(n_days):\n",
    "            pred = model.predict(np.array([current_sequence]))[0]  # Predict on the current sequence\n",
    "            rolling_predictions.append(pred)\n",
    "\n",
    "            # Reshape the prediction to ensure it has compatible dimensions for concatenation\n",
    "            pred_reshaped = pred[:len(features)].reshape(1, -1)  # Reshape prediction to 2D\n",
    "\n",
    "            # Append the prediction to the sequence and remove the oldest time step\n",
    "            next_input = np.vstack([current_sequence[1:], pred_reshaped])\n",
    "            current_sequence = next_input\n",
    "\n",
    "        rolling_predictions = np.array(rolling_predictions)\n",
    "        return scaler_target.inverse_transform(rolling_predictions)  # Inverse transform the predictions\n",
    "\n",
    "    # Prepare for rolling predictions (using the last valid sequence)\n",
    "    last_sequence = combined_data_scaled.iloc[first_missing_loc-sequence_length:first_missing_loc][features].values\n",
    "\n",
    "    # Perform the rolling forecast for 8 days\n",
    "    rolling_predictions = rolling_forecast(model, last_sequence, 8, scaler_features, scaler_target)\n",
    "\n",
    "    # Create a DataFrame with the rolling predictions\n",
    "    predicted_dates = pd.date_range(start=first_missing_index, periods=8)\n",
    "    predicted_values_df = pd.DataFrame(rolling_predictions, columns=target, index=predicted_dates)\n",
    "\n",
    "    # Update the original combined_data DataFrame with the rolling predictions\n",
    "    combined_data.update(predicted_values_df)\n",
    "\n",
    "    print(\"predecting air quality data...\")\n",
    "  \n",
    "    # Function to categorize based on the pollutant dictionary\n",
    "    def categorize_value(value, pollutant):\n",
    "        for category, (low, high) in pollutant_categories[pollutant].items():\n",
    "            if low <= value < high:\n",
    "                return category\n",
    "        return None\n",
    "\n",
    "    # Example pollutant_categories dictionary\n",
    "    pollutant_categories = {\n",
    "        'CO': {'Good': (0, 1), 'Fair': (1, 2), 'Poor': (2, 5), 'Very Poor': (5, float('inf'))},\n",
    "        'OZONE': {'Good': (0, 5.4), 'Fair': (5.5, 8), 'Poor': (8.1, 12), 'Very Poor': (12.1, float('inf'))},\n",
    "        'PM10': {'Good': (0, 50), 'Fair': (50, 100), 'Poor': (100, 150), 'Very Poor': (150, float('inf'))},\n",
    "        'PM2.5': {'Good': (0, 25), 'Fair': (25, 50), 'Poor': (50, 75), 'Very Poor': (75, float('inf'))},\n",
    "        'SO2': {'Good': (0, 13.3), 'Fair': (13.4, 20), 'Poor': (20.1, 30), 'Very Poor': (30.1, float('inf'))}\n",
    "    }\n",
    "\n",
    "    # Example pollutant_mapping dictionary\n",
    "    pollutant_mapping = {\n",
    "        'CO_mean': 'CO',\n",
    "        'OZONE_mean': 'OZONE',\n",
    "        'PM10_mean': 'PM10',\n",
    "        'PM2.5_mean': 'PM2.5',\n",
    "        'SO2_mean': 'SO2'\n",
    "    }\n",
    "    pollutants = ['CO_mean', 'OZONE_mean', 'PM10_mean', 'PM2.5_mean', 'SO2_mean']\n",
    "\n",
    "    # Assuming your DataFrame contains the columns 'CO_mean', 'OZONE_mean', etc.\n",
    "    for pollutant in pollutants:\n",
    "        pollutant_key = pollutant_mapping[pollutant]  # Use the correct pollutant name in the dictionary\n",
    "        combined_data[f'{pollutant_key}'] = combined_data[pollutant].apply(lambda x: categorize_value(x, pollutant_key))\n",
    "\n",
    "    # Function to find the worst category for each row\n",
    "    def worst_category(row):\n",
    "        categories = row[[f'{pollutant_mapping[pollutant]}' for pollutant in pollutants]]\n",
    "        category_order = ['Very Poor', 'Poor', 'Fair', 'Good']\n",
    "        for cat in category_order:\n",
    "            if cat in categories.values:\n",
    "                return cat\n",
    "        return 'Good'\n",
    "\n",
    "    # Get today's date\n",
    "    today = pd.Timestamp.today().normalize()  # Normalize to get just the date (no time component)\n",
    "\n",
    "    # Filter the DataFrame for today and the next 6 days (including today)\n",
    "    next_7_days = combined_data[(combined_data.index >= today) & (combined_data.index < today + pd.Timedelta(days=7))].copy()\n",
    "\n",
    "    # Apply the categorization for the next 7 days and create the 'overall' column\n",
    "    next_7_days['overall'] = next_7_days.apply(worst_category, axis=1)\n",
    "\n",
    "    # Check if the 'overall' column exists, if not, create it in combined_data\n",
    "    if 'overall' not in combined_data.columns:\n",
    "        combined_data['overall'] = None\n",
    "\n",
    "    # Merge the next 7 days back into the original DataFrame (including the 'overall' column)\n",
    "    combined_data.update(next_7_days)\n",
    "\n",
    "    # Ensure the new 'overall' column is properly added and displayed in the DataFrame\n",
    "    combined_data.loc[next_7_days.index, 'overall'] = next_7_days['overall']\n",
    "\n",
    "    \n",
    "    print(\"categorise air quality data...\")\n",
    "\n",
    "    \n",
    "    today = pd.Timestamp.today().normalize()\n",
    "\n",
    "    next_7_days = today + pd.Timedelta(days=7)\n",
    "\n",
    "    week_data = combined_data.loc[(combined_data.index >= today) & (combined_data.index <= next_7_days)]\n",
    "\n",
    "\n",
    "        # Streamlit app: Sydney Air Quality Forecast\n",
    "    st.title(\"Sydney Air Quality Forecast\")\n",
    "    st.subheader(\"Weather\")\n",
    "\n",
    "    # Show Weather Data (TEMP_min, TEMP_max, RAIN_sum, forecast)\n",
    "    weather_columns = ['TEMP_min', 'TEMP_max', 'RAIN_sum', 'forecast']\n",
    "    st.write(week_data[weather_columns])\n",
    "\n",
    "    # Air Quality Subtitle\n",
    "    st.subheader(\"Air Quality\")\n",
    "\n",
    "    # Function for color coding air quality\n",
    "    def color_air_quality(val):\n",
    "        color = ''\n",
    "        if val == 'Good':\n",
    "            color = 'background-color: green'\n",
    "        elif val == 'Fair':\n",
    "            color = 'background-color: yellow'\n",
    "        elif val == 'Poor':\n",
    "            color = 'background-color: orange'\n",
    "        elif val == 'Very Poor':\n",
    "            color = 'background-color: red'\n",
    "        return color\n",
    "\n",
    "    # Select the air quality columns to display\n",
    "    air_quality_columns = ['CO', 'OZONE', 'PM10', 'PM2.5', 'SO2', 'overall']\n",
    "\n",
    "    # Apply the color formatting\n",
    "    styled_df = week_data[air_quality_columns].style.applymap(color_air_quality)\n",
    "\n",
    "    # Display the styled DataFrame in Streamlit\n",
    "    st.write(styled_df)\n",
    "    \n",
    "    print(\"update streamlit...\")\n",
    "\n",
    "    \n",
    "    # Finish the daily task\n",
    "    print(f\"Daily task completed at {datetime.now()}\")\n",
    "\n",
    "# Execute the daily task\n",
    "if __name__ == \"__main__\":\n",
    "    run_daily_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5430141",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m combined_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_data' is not defined"
     ]
    }
   ],
   "source": [
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51bba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
