{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "208e36fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 383\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_predictions\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# Run the main workflow\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m predictions \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[0;32mIn[26], line 362\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Fetch and process data\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     air_quality_df \u001b[38;5;241m=\u001b[39m fetch_air_quality_data()\n\u001b[1;32m    363\u001b[0m     daily_air_quality \u001b[38;5;241m=\u001b[39m process_air_quality_data(air_quality_df)\n\u001b[1;32m    364\u001b[0m     daily_traffic \u001b[38;5;241m=\u001b[39m process_traffic_data()\n",
      "Cell \u001b[0;32mIn[26], line 70\u001b[0m, in \u001b[0;36mfetch_air_quality_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m attempt \u001b[38;5;241m<\u001b[39m retries:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;66;03m# Process each iteration for the specified date chunk\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m         df \u001b[38;5;241m=\u001b[39m process_iteration(start_date_str, end_date_str)\n\u001b[1;32m     71\u001b[0m         data_frames\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData fetched from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 42\u001b[0m, in \u001b[0;36mprocess_iteration\u001b[0;34m(start_date_str, end_date_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPM10\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPM2.5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNH3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSO2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOZONE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTSPd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAIN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     33\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOLAR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEMP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSD1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWDR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWSP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHumid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEPH\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourly Average\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     40\u001b[0m }\n\u001b[1;32m     41\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m---> 42\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload))\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     45\u001b[0m     data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    488\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    489\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    490\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    491\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    492\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    496\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    497\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    498\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from ftplib import FTP\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "# Define the base URL for the POST request\n",
    "url = 'https://data.airquality.nsw.gov.au/api/Data/get_Observations'\n",
    "\n",
    "# Set start and end dates dynamically\n",
    "start_date = datetime.strptime(\"2015-04-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.today()  # End date is today\n",
    "max_chunk_days = 112  # Maximum number of days per API call\n",
    "retries = 3  # Retry attempts for failed iterations\n",
    "\n",
    "# Function to make the API request and process the data\n",
    "def process_iteration(start_date_str, end_date_str):\n",
    "    payload = {\n",
    "        \"Parameters\": [\"PM10\", \"PM2.5\", \"CO\", \"NH3\", \"NO\", \"NO2\", \"SO2\", \"OZONE\", \"TSPd\", \"RAIN\", \n",
    "                       \"SOLAR\", \"TEMP\", \"SD1\", \"WDR\", \"WSP\", \"Humid\", \"NEPH\"],\n",
    "        \"Sites\": [39],  # List of site IDs\n",
    "        \"StartDate\": start_date_str,\n",
    "        \"EndDate\": end_date_str,\n",
    "        \"Categories\": [\"Averages\"],\n",
    "        \"SubCategories\": [\"Hourly\"],\n",
    "        \"Frequency\": [\"Hourly Average\"]\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Fetch Air Quality Data dynamically with automatic chunking\n",
    "def fetch_air_quality_data():\n",
    "    global start_date, end_date  # Use global to modify the start_date/end_date if necessary\n",
    "    data_frames = []\n",
    "    \n",
    "    # Continue fetching data until start_date exceeds end_date\n",
    "    while start_date < end_date:\n",
    "        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date_chunk = start_date + timedelta(days=max_chunk_days)\n",
    "        \n",
    "        # Ensure end_date_chunk doesn't exceed today's date\n",
    "        if end_date_chunk > end_date:\n",
    "            end_date_chunk = end_date\n",
    "        \n",
    "        end_date_str = end_date_chunk.strftime(\"%Y-%m-%d\")\n",
    "        attempt = 0\n",
    "        \n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                # Process each iteration for the specified date chunk\n",
    "                df = process_iteration(start_date_str, end_date_str)\n",
    "                data_frames.append(df)\n",
    "                print(f\"Data fetched from {start_date_str} to {end_date_str}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error fetching data: {e}. Retrying...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Update start_date to be the day after the current chunk\n",
    "        start_date = end_date_chunk + timedelta(days=1)\n",
    "        time.sleep(2)  # Sleep to avoid hitting rate limits\n",
    "    \n",
    "    # Combine all the fetched data into a single DataFrame\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    combined_df = combined_df.infer_objects()  # Ensure data types are inferred correctly\n",
    "    print(f\"Air quality data collection completed at {datetime.now()}\")\n",
    "    combined_df.to_csv('combined_df.csv')\n",
    "    return combined_df\n",
    "\n",
    "# Process air quality data to daily aggregation\n",
    "def process_air_quality_data(df):\n",
    "    df['ParameterCode'] = df['Parameter'].apply(lambda x: x.get('ParameterCode') if isinstance(x, dict) else None)\n",
    "    df['ParameterDescription'] = df['Parameter'].apply(lambda x: x.get('ParameterDescription') if isinstance(x, dict) else None)\n",
    "\n",
    "    df_wide = df.pivot_table(index=['Site_Id', 'Date', 'Hour', 'HourDescription'],\n",
    "                             columns='ParameterCode', \n",
    "                             values='Value', \n",
    "                             aggfunc='first').reset_index()\n",
    "\n",
    "    df_wide.interpolate(method='linear', axis=0, inplace=True)\n",
    "    df_wide['datetime'] = pd.to_datetime(df_wide['Date']) + pd.to_timedelta(df_wide['Hour'], unit='h')\n",
    "    df_wide.set_index('datetime', inplace=True)\n",
    "\n",
    "    aggregation_rules_mean = {\n",
    "        'CO': 'mean', 'HUMID': 'mean', 'NEPH': 'mean', 'NO': 'mean', 'NO2': 'mean', 'OZONE': 'mean', \n",
    "        'SO2': 'mean', 'PM10': 'mean', 'PM2.5': 'mean', 'RAIN': 'sum', 'TEMP': ['min', 'max'], \n",
    "        'WSP': 'max', 'SD1': 'mean', 'WDR': 'mean'\n",
    "    }\n",
    "    daily_aggregated_mean = df_wide.resample('D').agg(aggregation_rules_mean)\n",
    "    daily_aggregated_mean.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in daily_aggregated_mean.columns]\n",
    "    daily_aggregated_mean.interpolate(method='linear', inplace=True)\n",
    "\n",
    "    return daily_aggregated_mean\n",
    "\n",
    "# Process traffic data\n",
    "def process_traffic_data():\n",
    "    df = pd.read_csv('/Users/evankerivan/Desktop/IOD/capstone/traffic_victoria_road.csv')\n",
    "    \n",
    "    df_melted = pd.melt(df, \n",
    "                        id_vars=['year', 'date', 'cardinal_direction_seq', 'classification_seq', 'public_holiday', 'school_holiday'],\n",
    "                        value_vars=[f'hour_{str(i).zfill(2)}' for i in range(24)],\n",
    "                        var_name='hour', \n",
    "                        value_name='traffic_count')\n",
    "\n",
    "    df_melted['hour'] = df_melted['hour'].str.replace('hour_', '').astype(int)\n",
    "    df_pivoted = df_melted.pivot_table(index=['year', 'date', 'hour', 'public_holiday', 'school_holiday'],\n",
    "                                       columns='classification_seq', \n",
    "                                       values='traffic_count').reset_index()\n",
    "\n",
    "    df_pivoted.rename(columns={'Heavy Vehicles': 'heavy_vehicle', 'Light Vehicles': 'light_vehicle'}, inplace=True)\n",
    "    df_clean = df_pivoted.dropna()\n",
    "\n",
    "    df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n",
    "    df_clean.set_index('datetime', inplace=True)\n",
    "\n",
    "    daily_traffic_data = df_clean.resample('D').agg({\n",
    "        'public_holiday': 'max', 'school_holiday': 'max', 'heavy_vehicle': 'sum', 'light_vehicle': 'sum'\n",
    "    })\n",
    "\n",
    "    return daily_traffic_data\n",
    "\n",
    "# LSTM to impute missing traffic data and predict 7 days ahead\n",
    "def lstm_traffic_imputation(daily_traffic_data):\n",
    "    # Handle missing values before scaling\n",
    "    daily_traffic_data = daily_traffic_data.interpolate().fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Scale the traffic data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(daily_traffic_data[['heavy_vehicle', 'light_vehicle']])\n",
    "\n",
    "    # Prepare sequences (7-day sequences for LSTM)\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])\n",
    "            y.append(data[i])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    sequence_length = 7\n",
    "    X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, X.shape[2])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(2))  # Predict heavy_vehicle and light_vehicle\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Predict missing traffic data for 7 days\n",
    "    last_sequence = X_test[-1]\n",
    "    predictions = []\n",
    "    for _ in range(7):  # Predict 7 days ahead\n",
    "        pred = model.predict(np.expand_dims(last_sequence, axis=0))\n",
    "        predictions.append(pred[0])\n",
    "        last_sequence = np.vstack([last_sequence[1:], pred[0]])\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    return pd.DataFrame(predictions, columns=['heavy_vehicle', 'light_vehicle'], index=pd.date_range(start=daily_traffic_data.index[-1], periods=7))\n",
    "\n",
    "# Feature selection using RFE\n",
    "\n",
    "\n",
    "# Function for feature selection with missing value handling\n",
    "def feature_selection(X, y):\n",
    "    # Fill missing values (NaNs) in X and y using interpolation or drop NaNs\n",
    "    X = X.interpolate().fillna(method='bfill').fillna(method='ffill')  # Interpolate, then backfill and forward fill\n",
    "    y = y.interpolate().fillna(method='bfill').fillna(method='ffill')  # Interpolate, then backfill and forward fill\n",
    "    \n",
    "    # Alternatively, you can use dropna if you want to remove rows with NaN values\n",
    "    # X = X.dropna()\n",
    "    # y = y.dropna()\n",
    "\n",
    "    # RandomForest for feature selection\n",
    "    model = RandomForestRegressor()\n",
    "    rfe = RFE(model, n_features_to_select=5)  # Adjust n_features_to_select as needed\n",
    "    fit = rfe.fit(X, y)\n",
    "\n",
    "    # Print and return selected features\n",
    "    selected_features = [f for f, selected in zip(X.columns, fit.support_) if selected]\n",
    "    print(f\"Selected Features: {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# Create LSTM model with parameters for grid search\n",
    "def create_lstm_model(units=64, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, return_sequences=True, input_shape=(7, len(selected_features))))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(Dense(len(target_columns)))  # Output air quality features\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Grid search for LSTM model parameters\n",
    "def grid_search_lstm(X_train, y_train):\n",
    "    model = KerasRegressor(build_fn=create_lstm_model, verbose=0)\n",
    "    param_grid = {\n",
    "        'units': [32, 64, 128],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'epochs': [10, 20],\n",
    "        'optimizer': ['adam', 'rmsprop']\n",
    "    }\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=3))\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    print(f\"Best Params: {grid_result.best_params_}\")\n",
    "    return grid_result.best_estimator_\n",
    "\n",
    "# LSTM for air quality prediction using weather, traffic, and air quality data\n",
    "def lstm_air_quality_prediction(combined_data):\n",
    "    # Handle missing values before scaling\n",
    "    combined_data = combined_data.interpolate().fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "    # Define features and target columns\n",
    "    features = ['TEMP_max', 'TEMP_min', 'RAIN_sum', 'WSP_max', 'WDR_mean', 'heavy_vehicle', 'light_vehicle']  # Add other features if needed\n",
    "    target_columns = ['PM10_mean', 'PM2.5_mean', 'CO_mean', 'NO_mean', 'SO2_mean']\n",
    "\n",
    "    # Perform feature selection\n",
    "    X = combined_data[features]\n",
    "    y = combined_data[target_columns]\n",
    "    selected_features = feature_selection(X, y)\n",
    "\n",
    "    # Scale the selected features and target\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X[selected_features])\n",
    "    y_scaled = scaler.fit_transform(y)\n",
    "\n",
    "    # Prepare sequences (7-day sequences for LSTM)\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])\n",
    "            y.append(data[i])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    sequence_length = 7\n",
    "    X_seq, y_seq = create_sequences(X_scaled, sequence_length)\n",
    "\n",
    "    # Split the data into training and testing\n",
    "    split_index = int(len(X_seq) * 0.8)\n",
    "    X_train, X_test = X_seq[:split_index], X_seq[split_index:]\n",
    "    y_train, y_test = y_seq[:split_index], y_seq[split_index:]\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(y_train.shape[1]))  # Output for all target variables\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Predict air quality data for 7 days\n",
    "    last_sequence = X_test[-1]\n",
    "    predictions = []\n",
    "    for _ in range(7):  # Predict 7 days ahead\n",
    "        pred = model.predict(np.expand_dims(last_sequence, axis=0))\n",
    "        predictions.append(pred[0])\n",
    "        last_sequence = np.vstack([last_sequence[1:], pred[0]])\n",
    "\n",
    "    # Inverse transform the predictions\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    return pd.DataFrame(predictions, columns=target_columns, index=pd.date_range(start=combined_data.index[-1], periods=7))\n",
    "\n",
    "# Process weather forecast data\n",
    "def process_weather_forecast():\n",
    "    # Connect to FTP and download the XML file\n",
    "    ftp = FTP('ftp.bom.gov.au')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/anon/gen/fwo/')\n",
    "    with open('IDN11060.xml', 'wb') as file:\n",
    "        ftp.retrbinary('RETR IDN11060.xml', file.write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse('IDN11060.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Prepare an empty list to store the data\n",
    "    data = []\n",
    "    for area in root.findall('.//area'):\n",
    "        location = area.attrib.get('description')\n",
    "        for period in area.findall('.//forecast-period'):\n",
    "            start_time = period.attrib.get('start-time-local')\n",
    "            forecast_data = {'Location': location, 'Date': start_time}\n",
    "            for element in period.findall('element'):\n",
    "                param_type = element.attrib.get('type')\n",
    "                value = element.text\n",
    "                units = element.attrib.get('units', '')\n",
    "                forecast_data[f'{param_type} ({units})'] = value\n",
    "            for text in period.findall('text'):\n",
    "                text_type = text.attrib.get('type')\n",
    "                text_value = text.text\n",
    "                forecast_data[text_type] = text_value\n",
    "            data.append(forecast_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[['Location', 'Date', 'air_temperature_maximum (Celsius)', 'air_temperature_minimum (Celsius)', 'precipitation_range ()']].copy()\n",
    "\n",
    "    df.columns = ['Location', 'Date', 'Temp_Max', 'Temp_Min', 'Rain']\n",
    "    df['Rain'] = df['Rain'].apply(lambda rain: max(map(int, re.findall(r'\\d+', rain))) if pd.notna(rain) else 0)\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    df['Temp_Max'] = pd.to_numeric(df['Temp_Max'], errors='coerce')\n",
    "    df['Temp_Min'] = pd.to_numeric(df['Temp_Min'], errors='coerce')\n",
    "\n",
    "    avg_temp_diff = df['Temp_Max'] - df['Temp_Min']\n",
    "    avg_temp_diff = avg_temp_diff.mean()\n",
    "    df['Temp_Min'].fillna(df['Temp_Max'] - avg_temp_diff, inplace=True)\n",
    "    \n",
    "    return df[df['Location'] == 'Sydney'][['Date', 'Temp_Max', 'Temp_Min', 'Rain']].copy()\n",
    "\n",
    "# Combine all data (traffic, weather, and air quality)\n",
    "def combine_all_data(daily_air_quality, daily_traffic, daily_forecast):\n",
    "    # Convert 'Date' column in daily_forecast to datetime to ensure uniformity in index\n",
    "    daily_forecast['Date'] = pd.to_datetime(daily_forecast['Date'])\n",
    "    \n",
    "    # Set 'Date' as the index for daily_forecast if it's not already\n",
    "    daily_forecast = daily_forecast.set_index('Date')\n",
    "    \n",
    "    # Combine all the data using outer join\n",
    "    combined_data = pd.concat([daily_air_quality, daily_traffic, daily_forecast], axis=1, join='outer')\n",
    "    \n",
    "    # Ensure the index is in datetime format\n",
    "    combined_data.index = pd.to_datetime(combined_data.index, errors='coerce')\n",
    "\n",
    "    # Sort the index after ensuring it's all datetime\n",
    "    combined_data = combined_data.sort_index()\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Fetch and process data\n",
    "    air_quality_df = fetch_air_quality_data()\n",
    "    daily_air_quality = process_air_quality_data(air_quality_df)\n",
    "    daily_traffic = process_traffic_data()\n",
    "    daily_forecast = process_weather_forecast()\n",
    "\n",
    "    # Combine all data\n",
    "    combined_data = combine_all_data(daily_air_quality, daily_traffic, daily_forecast)\n",
    "\n",
    "    # LSTM traffic imputation for the next 7 days\n",
    "    traffic_predictions = lstm_traffic_imputation(daily_traffic)\n",
    "\n",
    "    # LSTM air quality prediction for the next 7 days\n",
    "    air_quality_predictions = lstm_air_quality_prediction(combined_data)\n",
    "\n",
    "    combined_predictions = pd.concat([traffic_predictions, air_quality_predictions], axis=1)\n",
    "    print(\"Combined predictions for the next 7 days:\")\n",
    "    print(combined_predictions)\n",
    "\n",
    "    return combined_predictions\n",
    "\n",
    "# Run the main workflow\n",
    "predictions = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff8e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df=pd.read_csv('combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ac96227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Site_Id</th>\n",
       "      <th>Parameter</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>HourDescription</th>\n",
       "      <th>Value</th>\n",
       "      <th>AirQualityCategory</th>\n",
       "      <th>DeterminingPollutant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>{'ParameterCode': 'CO', 'ParameterDescription'...</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12 am - 1 am</td>\n",
       "      <td>0.754289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>{'ParameterCode': 'HUMID', 'ParameterDescripti...</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12 am - 1 am</td>\n",
       "      <td>94.199000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>{'ParameterCode': 'NEPH', 'ParameterDescriptio...</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12 am - 1 am</td>\n",
       "      <td>0.232000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>{'ParameterCode': 'NO', 'ParameterDescription'...</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12 am - 1 am</td>\n",
       "      <td>6.026604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>{'ParameterCode': 'NO2', 'ParameterDescription...</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>12 am - 1 am</td>\n",
       "      <td>1.959503</td>\n",
       "      <td>GOOD</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Site_Id                                          Parameter  \\\n",
       "0           0       39  {'ParameterCode': 'CO', 'ParameterDescription'...   \n",
       "1           1       39  {'ParameterCode': 'HUMID', 'ParameterDescripti...   \n",
       "2           2       39  {'ParameterCode': 'NEPH', 'ParameterDescriptio...   \n",
       "3           3       39  {'ParameterCode': 'NO', 'ParameterDescription'...   \n",
       "4           4       39  {'ParameterCode': 'NO2', 'ParameterDescription...   \n",
       "\n",
       "         Date  Hour HourDescription      Value AirQualityCategory  \\\n",
       "0  2015-04-01     1    12 am - 1 am   0.754289                NaN   \n",
       "1  2015-04-01     1    12 am - 1 am  94.199000                NaN   \n",
       "2  2015-04-01     1    12 am - 1 am   0.232000                NaN   \n",
       "3  2015-04-01     1    12 am - 1 am   6.026604                NaN   \n",
       "4  2015-04-01     1    12 am - 1 am   1.959503               GOOD   \n",
       "\n",
       "   DeterminingPollutant  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d6348d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the base URL for the POST request\n",
    "url = 'https://data.airquality.nsw.gov.au/api/Data/get_Observations'\n",
    "\n",
    "# Set start and end dates dynamically\n",
    "start_date = datetime.strptime(\"2015-04-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.today()  # End date is today\n",
    "max_chunk_days = 112  # Maximum number of days per API call\n",
    "retries = 3  # Retry attempts for failed iterations\n",
    "\n",
    "# Function to make the API request and process the data\n",
    "def process_iteration(start_date_str, end_date_str):\n",
    "    payload = {\n",
    "        \"Parameters\": [\"PM10\", \"PM2.5\", \"CO\", \"NH3\", \"NO\", \"NO2\", \"SO2\", \"OZONE\", \"TSPd\", \"RAIN\", \n",
    "                       \"SOLAR\", \"TEMP\", \"SD1\", \"WDR\", \"WSP\", \"Humid\", \"NEPH\"],\n",
    "        \"Sites\": [39],  # List of site IDs\n",
    "        \"StartDate\": start_date_str,\n",
    "        \"EndDate\": end_date_str,\n",
    "        \"Categories\": [\"Averages\"],\n",
    "        \"SubCategories\": [\"Hourly\"],\n",
    "        \"Frequency\": [\"Hourly Average\"]\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "# Fetch Air Quality Data dynamically with automatic chunking\n",
    "def fetch_air_quality_data():\n",
    "    global start_date, end_date  # Use global to modify the start_date/end_date if necessary\n",
    "    data_frames = []\n",
    "    \n",
    "    # Continue fetching data until start_date exceeds end_date\n",
    "    while start_date < end_date:\n",
    "        start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date_chunk = start_date + timedelta(days=max_chunk_days)\n",
    "        \n",
    "        # Ensure end_date_chunk doesn't exceed today's date\n",
    "        if end_date_chunk > end_date:\n",
    "            end_date_chunk = end_date\n",
    "        \n",
    "        end_date_str = end_date_chunk.strftime(\"%Y-%m-%d\")\n",
    "        attempt = 0\n",
    "        \n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                # Process each iteration for the specified date chunk\n",
    "                df = process_iteration(start_date_str, end_date_str)\n",
    "                data_frames.append(df)\n",
    "                print(f\"Data fetched from {start_date_str} to {end_date_str}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                print(f\"Error fetching data: {e}. Retrying...\")\n",
    "                time.sleep(5)\n",
    "\n",
    "        # Update start_date to be the day after the current chunk\n",
    "        start_date = end_date_chunk + timedelta(days=1)\n",
    "        time.sleep(2)  # Sleep to avoid hitting rate limits\n",
    "    \n",
    "    # Combine all the fetched data into a single DataFrame\n",
    "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "    combined_df = combined_df.infer_objects()  # Ensure data types are inferred correctly\n",
    "    print(f\"Air quality data collection completed at {datetime.now()}\")\n",
    "    combined_df.to_csv('combined_df.csv')\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec2cd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/kn0p5yrn6d771z_m2v7qll780000gp/T/ipykernel_37325/3603239885.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['forecast'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 211\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Run the workflow\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 211\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[27], line 198\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m daily_traffic \u001b[38;5;241m=\u001b[39m process_traffic_data()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Fetch and process weather forecast data\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m forecasted_weather \u001b[38;5;241m=\u001b[39m process_weather_forecast()\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# Combine data\u001b[39;00m\n\u001b[1;32m    201\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([daily_air_quality, daily_traffic, forecasted_weather], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 82\u001b[0m, in \u001b[0;36mprocess_weather_forecast\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Extract the relevant columns for weather data\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mair_temperature_maximum (Celsius)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mair_temperature_minimum (Celsius)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitation_range ()\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforecast\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     83\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Max\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Min\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Process temperature and precipitation data\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['forecast'] not in index\""
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from ftplib import FTP\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "import re\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "# Process traffic data\n",
    "def process_traffic_data():\n",
    "    df = pd.read_csv('/Users/evankerivan/Desktop/IOD/capstone/traffic_victoria_road.csv')\n",
    "    df_melted = pd.melt(df, \n",
    "                        id_vars=['year', 'date', 'cardinal_direction_seq', 'classification_seq', 'public_holiday', 'school_holiday'],\n",
    "                        value_vars=[f'hour_{str(i).zfill(2)}' for i in range(24)],\n",
    "                        var_name='hour', \n",
    "                        value_name='traffic_count')\n",
    "\n",
    "    df_melted['hour'] = df_melted['hour'].str.replace('hour_', '').astype(int)\n",
    "    df_pivoted = df_melted.pivot_table(index=['year', 'date', 'hour', 'public_holiday', 'school_holiday'],\n",
    "                                       columns='classification_seq', \n",
    "                                       values='traffic_count').reset_index()\n",
    "\n",
    "    df_pivoted.rename(columns={'Heavy Vehicles': 'heavy_vehicle', 'Light Vehicles': 'light_vehicle'}, inplace=True)\n",
    "    df_clean = df_pivoted.dropna()\n",
    "\n",
    "    df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n",
    "    df_clean.set_index('datetime', inplace=True)\n",
    "\n",
    "    daily_traffic_data = df_clean.resample('D').agg({\n",
    "        'public_holiday': 'max', 'school_holiday': 'max', 'heavy_vehicle': 'sum', 'light_vehicle': 'sum'\n",
    "    })\n",
    "\n",
    "    return daily_traffic_data\n",
    "\n",
    "# Function to parse weather data and extract wind speed and direction\n",
    "def process_weather_forecast():\n",
    "    # Connect to FTP and download the XML file\n",
    "    ftp = FTP('ftp.bom.gov.au')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/anon/gen/fwo/')\n",
    "    with open('IDN11060.xml', 'wb') as file:\n",
    "        ftp.retrbinary('RETR IDN11060.xml', file.write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse('IDN11060.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Prepare an empty list to store the data\n",
    "    data = []\n",
    "    for area in root.findall('.//area'):\n",
    "        location = area.attrib.get('description')\n",
    "        for period in area.findall('.//forecast-period'):\n",
    "            start_time = period.attrib.get('start-time-local')\n",
    "            forecast_data = {'Location': location, 'Date': start_time}\n",
    "            for element in period.findall('element'):\n",
    "                param_type = element.attrib.get('type')\n",
    "                value = element.text\n",
    "                units = element.attrib.get('units', '')\n",
    "                forecast_data[f'{param_type} ({units})'] = value\n",
    "            for text in period.findall('text'):\n",
    "                text_type = text.attrib.get('type')\n",
    "                text_value = text.text\n",
    "                forecast_data[text_type] = text_value\n",
    "            data.append(forecast_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract the relevant columns for weather data\n",
    "    df = df[['Location', 'Date', 'air_temperature_maximum (Celsius)', 'air_temperature_minimum (Celsius)', 'precipitation_range ()', 'forecast']].copy()\n",
    "    df.columns = ['Location', 'Date', 'Temp_Max', 'Temp_Min', 'Rain', 'Forecast']\n",
    "\n",
    "    # Process temperature and precipitation data\n",
    "    df['Rain'] = df['Rain'].apply(lambda rain: max(map(int, re.findall(r'\\d+', rain))) if pd.notna(rain) else 0)\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    df['Temp_Max'] = pd.to_numeric(df['Temp_Max'], errors='coerce')\n",
    "    df['Temp_Min'] = pd.to_numeric(df['Temp_Min'], errors='coerce')\n",
    "\n",
    "    avg_temp_diff = df['Temp_Max'] - df['Temp_Min']\n",
    "    avg_temp_diff = avg_temp_diff.mean()\n",
    "    df['Temp_Min'].fillna(df['Temp_Max'] - avg_temp_diff, inplace=True)\n",
    "\n",
    "    # Extract wind information (direction and speed)\n",
    "    df['Wind_Direction'], df['Wind_Speed_Min'], df['Wind_Speed_Max'] = zip(*df['Forecast'].apply(extract_wind_info))\n",
    "    \n",
    "    # Convert wind direction to degrees\n",
    "    df['Wind_Direction_Degrees'] = df['Wind_Direction'].apply(wind_direction_to_degrees)\n",
    "\n",
    "    # Convert wind speed from km/h to m/s\n",
    "    df['Wind_Speed_Min (m/s)'] = df['Wind_Speed_Min'].apply(kmh_to_ms)\n",
    "    df['Wind_Speed_Max (m/s)'] = df['Wind_Speed_Max'].apply(kmh_to_ms)\n",
    "\n",
    "    # Filter the data for Sydney\n",
    "    df_sydney = df[df['Location'] == 'Sydney'].copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_sydney.drop(columns=['Location', 'Forecast'], inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format and set it as the index\n",
    "    df_sydney['Date'] = pd.to_datetime(df_sydney['Date'])\n",
    "    df_sydney.set_index('Date', inplace=True)\n",
    "\n",
    "    # Rename columns to match desired format\n",
    "    df_sydney.rename(columns={\n",
    "        'Temp_Max': 'TEMP_max',\n",
    "        'Temp_Min': 'TEMP_min',\n",
    "        'Rain': 'RAIN_sum',\n",
    "        'Wind_Direction_Degrees': 'WDR_mean',\n",
    "        'Wind_Speed_Max (m/s)': 'WSP_max'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df_sydney[['TEMP_max', 'TEMP_min', 'RAIN_sum', 'WSP_max', 'WDR_mean']]\n",
    "\n",
    "# LSTM model to predict air quality based on past air quality, weather, and traffic data\n",
    "def lstm_air_quality_prediction(combined_data, forecasted_weather, traffic_predictions):\n",
    "    features = ['TEMP_min', 'TEMP_max', 'RAIN_sum', 'WSP_max', 'WDR_mean', 'heavy_vehicle', 'light_vehicle', \n",
    "                'PM10_mean', 'PM2.5_mean', 'OZONE_mean']\n",
    "    target_columns = ['PM10_mean', 'PM2.5_mean', 'OZONE_mean']\n",
    "\n",
    "    # Feature selection can be applied here if necessary\n",
    "    X = combined_data[features]\n",
    "    y = combined_data[target_columns]\n",
    "    \n",
    "    # Handling missing values\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    y.fillna(y.mean(), inplace=True)\n",
    "\n",
    "    # Scaling the data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Prepare sequences (for LSTM input)\n",
    "    sequence_length = 7\n",
    "    def create_sequences(data, seq_length):\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X_seq.append(data[i-seq_length:i])\n",
    "            y_seq.append(data[i])\n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    X_seq, y_seq = create_sequences(X_scaled, sequence_length)\n",
    "\n",
    "    # Split into training and testing sets (80% train, 20% test)\n",
    "    split_idx = int(len(X_seq) * 0.8)\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(len(target_columns)))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the LSTM model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Predict for the next 7 days using forecasted weather and traffic\n",
    "    last_sequence = X_test[-1]  # Start with the last available sequence\n",
    "    predictions = []\n",
    "    for _ in range(7):\n",
    "        pred = model.predict(np.expand_dims(last_sequence, axis=0))\n",
    "        predictions.append(pred[0])\n",
    "        last_sequence = np.vstack([last_sequence[1:], pred[0]])\n",
    "\n",
    "    return pd.DataFrame(predictions, columns=target_columns, index=pd.date_range(start=combined_data.index[-1], periods=7))\n",
    "\n",
    "# Main workflow to combine all components\n",
    "def main():\n",
    "    # Fetch and process air quality data\n",
    "    air_quality_df = pd.read_csv('combined_df.csv', parse_dates=['Date'], index_col='Date')\n",
    "    \n",
    "    numeric_columns = air_quality_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Resample and aggregate only numeric columns\n",
    "    daily_air_quality = air_quality_df[numeric_columns].resample('D').mean()\n",
    "\n",
    "    # Merge the non-numeric columns back if needed\n",
    "    non_numeric_columns = air_quality_df.select_dtypes(exclude=['number']).columns\n",
    "    \n",
    "    daily_air_quality = daily_air_quality.merge(air_quality_df[non_numeric_columns], left_index=True, right_index=True, how='left')\n",
    "    # Fetch and process traffic data\n",
    "    daily_traffic = process_traffic_data()\n",
    "\n",
    "    # Fetch and process weather forecast data\n",
    "    forecasted_weather = process_weather_forecast()\n",
    "\n",
    "    # Combine data\n",
    "    combined_data = pd.concat([daily_air_quality, daily_traffic, forecasted_weather], axis=1, join='outer')\n",
    "\n",
    "    # Make predictions for air quality using LSTM model\n",
    "    air_quality_predictions = lstm_air_quality_prediction(combined_data, forecasted_weather, daily_traffic)\n",
    "\n",
    "    print(\"Air Quality Predictions for the next 7 days:\")\n",
    "    print(air_quality_predictions)\n",
    "\n",
    "# Run the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1d5386f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "914dc70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/br/kn0p5yrn6d771z_m2v7qll780000gp/T/ipykernel_37325/3241065228.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Location', 'Date', 'forecast_icon_code ()', 'precis',\n",
      "       'probability_of_precipitation', 'air_temperature_minimum (Celsius)',\n",
      "       'air_temperature_maximum (Celsius)', 'precipitation_range ()'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Run the workflow\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[29], line 133\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m daily_traffic \u001b[38;5;241m=\u001b[39m process_traffic_data()\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Fetch and process weather forecast data\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m forecasted_weather \u001b[38;5;241m=\u001b[39m process_weather_forecast()\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Combine data\u001b[39;00m\n\u001b[1;32m    136\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([daily_air_quality, daily_traffic, forecasted_weather], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, join\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 84\u001b[0m, in \u001b[0;36mprocess_weather_forecast\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Min\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Max\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m avg_temp_diff, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Extract wind information (direction and speed)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind_Direction\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind_Speed_Min\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind_Speed_Max\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_wind_info) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mForecast\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Convert wind direction to degrees\u001b[39;00m\n\u001b[1;32m     87\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind_Direction_Degrees\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWind_Direction\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(wind_direction_to_degrees)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Process traffic data\n",
    "def process_traffic_data():\n",
    "    df = pd.read_csv('/Users/evankerivan/Desktop/IOD/capstone/traffic_victoria_road.csv')\n",
    "    df_melted = pd.melt(df, \n",
    "                        id_vars=['year', 'date', 'cardinal_direction_seq', 'classification_seq', 'public_holiday', 'school_holiday'],\n",
    "                        value_vars=[f'hour_{str(i).zfill(2)}' for i in range(24)],\n",
    "                        var_name='hour', \n",
    "                        value_name='traffic_count')\n",
    "\n",
    "    df_melted['hour'] = df_melted['hour'].str.replace('hour_', '').astype(int)\n",
    "    df_pivoted = df_melted.pivot_table(index=['year', 'date', 'hour', 'public_holiday', 'school_holiday'],\n",
    "                                       columns='classification_seq', \n",
    "                                       values='traffic_count').reset_index()\n",
    "\n",
    "    df_pivoted.rename(columns={'Heavy Vehicles': 'heavy_vehicle', 'Light Vehicles': 'light_vehicle'}, inplace=True)\n",
    "    df_clean = df_pivoted.dropna()\n",
    "\n",
    "    df_clean.loc[:, 'datetime'] = pd.to_datetime(df_clean['date']) + pd.to_timedelta(df_clean['hour'], unit='h')\n",
    "    df_clean.set_index('datetime', inplace=True)\n",
    "\n",
    "    daily_traffic_data = df_clean.resample('D').agg({\n",
    "        'public_holiday': 'max', 'school_holiday': 'max', 'heavy_vehicle': 'sum', 'light_vehicle': 'sum'\n",
    "    })\n",
    "\n",
    "    return daily_traffic_data\n",
    "\n",
    "# Function to parse weather data and extract wind speed and direction\n",
    "def process_weather_forecast():\n",
    "    # Connect to FTP and download the XML file\n",
    "    ftp = FTP('ftp.bom.gov.au')\n",
    "    ftp.login()\n",
    "    ftp.cwd('/anon/gen/fwo/')\n",
    "    with open('IDN11060.xml', 'wb') as file:\n",
    "        ftp.retrbinary('RETR IDN11060.xml', file.write)\n",
    "    ftp.quit()\n",
    "\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse('IDN11060.xml')\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Prepare an empty list to store the data\n",
    "    data = []\n",
    "    for area in root.findall('.//area'):\n",
    "        location = area.attrib.get('description')\n",
    "        for period in area.findall('.//forecast-period'):\n",
    "            start_time = period.attrib.get('start-time-local')\n",
    "            forecast_data = {'Location': location, 'Date': start_time}\n",
    "            for element in period.findall('element'):\n",
    "                param_type = element.attrib.get('type')\n",
    "                value = element.text\n",
    "                units = element.attrib.get('units', '')\n",
    "                forecast_data[f'{param_type} ({units})'] = value\n",
    "            for text in period.findall('text'):\n",
    "                text_type = text.attrib.get('type')\n",
    "                text_value = text.text\n",
    "                forecast_data[text_type] = text_value\n",
    "            data.append(forecast_data)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Extract the relevant columns for weather data\n",
    "    # Checking column names with print(df.columns) to ensure \"forecast\" exists\n",
    "    print(df.columns)\n",
    "\n",
    "    # Adjusted to match actual column names in your dataset.\n",
    "    if 'forecast' in df.columns:\n",
    "        df = df[['Location', 'Date', 'air_temperature_maximum (Celsius)', 'air_temperature_minimum (Celsius)', 'precipitation_range ()', 'forecast']].copy()\n",
    "    else:\n",
    "        df = df[['Location', 'Date', 'air_temperature_maximum (Celsius)', 'air_temperature_minimum (Celsius)', 'precipitation_range ()']].copy()\n",
    "\n",
    "    df.columns = ['Location', 'Date', 'Temp_Max', 'Temp_Min', 'Rain']\n",
    "\n",
    "    # Process temperature and precipitation data\n",
    "    df['Rain'] = df['Rain'].apply(lambda rain: max(map(int, re.findall(r'\\d+', rain))) if pd.notna(rain) else 0)\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "    df['Temp_Max'] = pd.to_numeric(df['Temp_Max'], errors='coerce')\n",
    "    df['Temp_Min'] = pd.to_numeric(df['Temp_Min'], errors='coerce')\n",
    "\n",
    "    avg_temp_diff = df['Temp_Max'] - df['Temp_Min']\n",
    "    avg_temp_diff = avg_temp_diff.mean()\n",
    "    df['Temp_Min'].fillna(df['Temp_Max'] - avg_temp_diff, inplace=True)\n",
    "\n",
    "    # Extract wind information (direction and speed)\n",
    "    df['Wind_Direction'], df['Wind_Speed_Min'], df['Wind_Speed_Max'] = zip(*df['Forecast'].apply(extract_wind_info) if 'Forecast' in df.columns else (None, None, None))\n",
    "    \n",
    "    # Convert wind direction to degrees\n",
    "    df['Wind_Direction_Degrees'] = df['Wind_Direction'].apply(wind_direction_to_degrees)\n",
    "\n",
    "    # Convert wind speed from km/h to m/s\n",
    "    df['Wind_Speed_Min (m/s)'] = df['Wind_Speed_Min'].apply(kmh_to_ms)\n",
    "    df['Wind_Speed_Max (m/s)'] = df['Wind_Speed_Max'].apply(kmh_to_ms)\n",
    "\n",
    "    # Filter the data for Sydney\n",
    "    df_sydney = df[df['Location'] == 'Sydney'].copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_sydney.drop(columns=['Location', 'Forecast'], inplace=True)\n",
    "\n",
    "    # Convert 'Date' column to datetime format and set it as the index\n",
    "    df_sydney['Date'] = pd.to_datetime(df_sydney['Date'])\n",
    "    df_sydney.set_index('Date', inplace=True)\n",
    "\n",
    "    # Rename columns to match desired format\n",
    "    df_sydney.rename(columns={\n",
    "        'Temp_Max': 'TEMP_max',\n",
    "        'Temp_Min': 'TEMP_min',\n",
    "        'Rain': 'RAIN_sum',\n",
    "        'Wind_Direction_Degrees': 'WDR_mean',\n",
    "        'Wind_Speed_Max (m/s)': 'WSP_max'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return df_sydney[['TEMP_max', 'TEMP_min', 'RAIN_sum', 'WSP_max', 'WDR_mean']]\n",
    "\n",
    "# Main workflow to combine all components\n",
    "def main():\n",
    "    # Fetch and process air quality data\n",
    "    air_quality_df = pd.read_csv('combined_df.csv', parse_dates=['Date'], index_col='Date')\n",
    "    \n",
    "    numeric_columns = air_quality_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Resample and aggregate only numeric columns\n",
    "    daily_air_quality = air_quality_df[numeric_columns].resample('D').mean()\n",
    "\n",
    "    # Merge the non-numeric columns back if needed\n",
    "    non_numeric_columns = air_quality_df.select_dtypes(exclude=['number']).columns\n",
    "    \n",
    "    daily_air_quality = daily_air_quality.merge(air_quality_df[non_numeric_columns], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    # Fetch and process traffic data\n",
    "    daily_traffic = process_traffic_data()\n",
    "\n",
    "    # Fetch and process weather forecast data\n",
    "    forecasted_weather = process_weather_forecast()\n",
    "\n",
    "    # Combine data\n",
    "    combined_data = pd.concat([daily_air_quality, daily_traffic, forecasted_weather], axis=1, join='outer')\n",
    "\n",
    "    # Make predictions for air quality using LSTM model\n",
    "    air_quality_predictions = lstm_air_quality_prediction(combined_data, forecasted_weather, daily_traffic)\n",
    "\n",
    "    print(\"Air Quality Predictions for the next 7 days:\")\n",
    "    print(air_quality_predictions)\n",
    "\n",
    "# Run the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b0ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
